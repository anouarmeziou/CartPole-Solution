{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Py3 research env",
      "language": "python",
      "name": "py3_research"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "practice12_Reinforce_practice.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jknY7iKNhdx5",
        "colab_type": "text"
      },
      "source": [
        "### REINFORCE in PyTorch\n",
        "__This notebook is based on [Practical_RL week06](https://github.com/yandexdataschool/Practical_RL/tree/master/week06_policy_based) materials__\n",
        "\n",
        "Just like we did before for q-learning, this time we'll design a pytorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
        "\n",
        "Most of the code in this notebook is taken from approximate qlearning, so you'll find it more or less familiar and even simpler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxoGxzafhdx6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "0e2d1773-0433-4f53-9c18-f06299b9d34d"
      },
      "source": [
        "# # in google colab uncomment this\n",
        "\n",
        "import os\n",
        "\n",
        "os.system('apt-get install -y xvfb')\n",
        "os.system('wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/xvfb -O ../xvfb')\n",
        "os.system('apt-get install -y python-opengl ffmpeg')\n",
        "os.system('pip install pyglet==1.4.9')\n",
        "\n",
        "os.system('python -m pip install -U pygame --user')\n",
        "\n",
        "print('setup complete')\n",
        "\n",
        "# XVFB will be launched if you run on a server\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY = : 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "setup complete\n",
            "Starting virtual X frame buffer: Xvfb.\n",
            "env: DISPLAY=: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jbXS-Omhdx9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "94afe06c-c6bc-49a5-95f1-e6f839d5544b"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "env = gym.make(\"CartPole-v0\").env\n",
        "example_state = env.reset()\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fda76d71048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD8uDWijhdyA",
        "colab_type": "text"
      },
      "source": [
        "# Building the network for REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_C_927whdyB",
        "colab_type": "text"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states. Let's define such a model below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huoeZlYBhdyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUYFWGNfixcx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape = example_state.shape[0]\n",
        "n_actions = env.action_space.n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et8NV2oPhdyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape = example_state.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "# Build a simple neural network that predicts policy logits. \n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "model = nn.Sequential(\n",
        "  nn.Linear(input_shape, 16),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(16, 32),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(32, n_actions),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLx8PmoJjPXP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "93a33a68-3720-4f34-8062-8f5c04ca08d2"
      },
      "source": [
        "from torchsummary import summary\n",
        "summary(model, (4,), device='cpu')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                   [-1, 16]              80\n",
            "              ReLU-2                   [-1, 16]               0\n",
            "            Linear-3                   [-1, 32]             544\n",
            "              ReLU-4                   [-1, 32]               0\n",
            "            Linear-5                    [-1, 2]              66\n",
            "================================================================\n",
            "Total params: 690\n",
            "Trainable params: 690\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le3RHcJlhdyK",
        "colab_type": "text"
      },
      "source": [
        "#### Predict function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R23RSJ2hdyL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_probs(states):\n",
        "    \"\"\" \n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute logits, use softmax to get probability\n",
        "    states = torch.FloatTensor(states)\n",
        "    logits = model(states).detach()\n",
        "    probs = torch.softmax(logits, dim=-1).numpy()\n",
        "    return probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG5naN7NhdyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_states = np.array([env.reset() for _ in range(5)])\n",
        "test_probas = predict_probs(test_states)\n",
        "assert isinstance(\n",
        "    test_probas, np.ndarray), \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (\n",
        "    test_states.shape[0], env.action_space.n), \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1),\n",
        "                   1), \"probabilities do not sum to 1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whHF25G1hdyR",
        "colab_type": "text"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21qHfFrbhdyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_session(t_max=1000):\n",
        "    \"\"\" \n",
        "    play a full session with REINFORCE agent and train at the session end.\n",
        "    returns sequences of states, actions andrewards\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]))[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        a = np.random.choice(np.arange(n_actions), p=action_probs)\n",
        "         #< your code >\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYdm7qeghdyW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIMVVny-hdyZ",
        "colab_type": "text"
      },
      "source": [
        "### Computing cumulative rewards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvL2wAJbhdyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "   take a list of immediate rewards r(s,a) for the whole session\n",
        "   compute cumulative returns (a.k.a. G(s,a) in Sutton '16)\n",
        "   G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        " \n",
        "   The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
        "   and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        " \n",
        "   You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "   \"\"\"\n",
        "#     <your code here >\n",
        "    G = np.zeros(len(rewards))\n",
        "    G[-1] = rewards[-1]\n",
        "   \n",
        "    for idx in range(-2, -len(rewards)-1, -1):\n",
        "        G[idx] = rewards[idx] + gamma*G[idx+1]\n",
        "    return G # < array of cumulative rewards >"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzbzEKaShdyc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "50ba22a3-0539-467e-f7d9-73466432cdab"
      },
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9), [\n",
        "                   1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(get_cumulative_rewards(\n",
        "    [0, 0, 1, -2, 3, -4, 0], gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(get_cumulative_rewards(\n",
        "    [0, 0, 1, 2, 3, 4, 0], gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "looks good!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijeXl_Gihdye",
        "colab_type": "text"
      },
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
        "\n",
        "\n",
        "Following the REINFORCE algorithm, we can define our objective as follows: \n",
        "\n",
        "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
        "\n",
        "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CFT9pgOhdyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHhvh73nhdyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        " \n",
        " \n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "   Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "   Updates agent's weights by following the policy gradient above.\n",
        "   Please use Adam optimizer with default parameters.\n",
        "   \"\"\"\n",
        " \n",
        "    # cast everything into torch tensors\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        " \n",
        "    # predict logits, probas and log-probas using an agent.\n",
        "    logits = model(states)\n",
        "    probs = nn.functional.softmax(logits, -1)\n",
        "    log_probs = nn.functional.log_softmax(logits, -1)\n",
        " \n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        " \n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = torch.sum(\n",
        "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
        "   \n",
        "    # Compute loss here. Don't forget entropy regularization with `entropy_coef`\n",
        "    entropy = -torch.mean(torch.sum(probs * log_probs, dim=1)) # < your code >\n",
        "#     print(entropy.shape)\n",
        "    loss = -torch.mean(log_probs_for_actions * cumulative_returns) - entropy * entropy_coef  # < your code >\n",
        "#     print(loss.shape)\n",
        "    # Gradient descent step\n",
        "#     < your code >\n",
        "   \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "   \n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k17CwfxIhdyo",
        "colab_type": "text"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "2-ckq48ehdyp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "0a1bbcf6-655a-4321-99d6-2a4864d3f41d"
      },
      "source": [
        "for i in range(100):\n",
        "    rewards = [train_on_session(*generate_session())\n",
        "               for _ in range(100)]  # generate new sessions\n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "    if np.mean(rewards) > 500:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean reward:23.390\n",
            "mean reward:26.520\n",
            "mean reward:25.200\n",
            "mean reward:28.500\n",
            "mean reward:42.880\n",
            "mean reward:68.500\n",
            "mean reward:122.530\n",
            "mean reward:207.520\n",
            "mean reward:208.170\n",
            "mean reward:145.510\n",
            "mean reward:195.050\n",
            "mean reward:350.990\n",
            "mean reward:529.090\n",
            "You Win!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXwyQ0oHhdyr",
        "colab_type": "text"
      },
      "source": [
        "### Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od7t2GLFhdys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# record sessions\n",
        "import gym.wrappers\n",
        "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),\n",
        "                           directory=\"videos\", force=True)\n",
        "sessions = [generate_session() for _ in range(100)]\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx-OmV4Rhdyu",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/videos/openaigym.video.1.121.video000000.mp4": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 404,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "outputId": "ed023771-69e9-4376-f90b-842b44910123"
      },
      "source": [
        "# show video\n",
        "from IPython.display import HTML\n",
        "import os\n",
        "\n",
        "video_names = list(\n",
        "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be the _last_ video. Try other indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<video width=\"640\" height=\"480\" controls>\n",
              "  <source src=\"./videos/openaigym.video.1.121.video000000.mp4\" type=\"video/mp4\">\n",
              "</video>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o-gbZQrhdyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1uYouWshdy1",
        "colab_type": "text"
      },
      "source": [
        "### Bonus area: solving Acrobot-v1\n",
        "Try to solve more complex environment using Policy gradient method.\n",
        "*Hint: you will need add some imporovements to the original REINFORCE (e.g. Advantage Actor Critic or anything else).*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm1z4d2qhdy1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "e876dc97-e4af-40db-d65d-f223d338346c"
      },
      "source": [
        "env = gym.make(\"Acrobot-v1\")\n",
        "env.reset()\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))\n",
        "state_dim = env.reset().shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "print(state_dim, n_actions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VQ8yREchdy4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_shape = state_dim\n",
        "n_actions = env.action_space.n\n",
        "# Build a simple neural network that predicts policy logits. \n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "model = nn.Sequential(\n",
        "  nn.Linear(input_shape, 16),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(16, 32),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(32, n_actions),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogCraaPzr-nU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        " \n",
        " \n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "   Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "   Updates agent's weights by following the policy gradient above.\n",
        "   Please use Adam optimizer with default parameters.\n",
        "   \"\"\"\n",
        " \n",
        "    # cast everything into torch tensors\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        " \n",
        "    # predict logits, probas and log-probas using an agent.\n",
        "    logits = model(states)\n",
        "    probs = nn.functional.softmax(logits, -1)\n",
        "    log_probs = nn.functional.log_softmax(logits, -1)\n",
        " \n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        " \n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = torch.sum(\n",
        "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
        "   \n",
        "    # Compute loss here. Don't forget entropy regularization with `entropy_coef`\n",
        "    entropy = -torch.mean(torch.sum(probs * log_probs, dim=1)) # < your code >\n",
        "#     print(entropy.shape)\n",
        "    loss = -torch.mean(log_probs_for_actions * (cumulative_returns - cumulative_returns.mean())) - entropy * entropy_coef  # < your code >\n",
        "#     print(loss.shape)\n",
        "    # Gradient descent step\n",
        "#     < your code >\n",
        "   \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "   \n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejpJtWQmq_Ot",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        },
        "outputId": "f0b4e40c-9c18-4dfa-f221-d2b005821214"
      },
      "source": [
        "entropy_coef = 1e-2\n",
        "for i in range(100):\n",
        "    rewards = [train_on_session(*generate_session(), entropy_coef=entropy_coef)\n",
        "               for _ in range(100)]  # generate new sessions\n",
        "    if i %10 == 0:\n",
        "      entropy_coef *= 0.7\n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "    if np.mean(rewards) > -100: \n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean reward:-500.000\n",
            "mean reward:-500.000\n",
            "mean reward:-500.000\n",
            "mean reward:-500.000\n",
            "mean reward:-500.000\n",
            "mean reward:-500.000\n",
            "mean reward:-500.000\n",
            "mean reward:-500.000\n",
            "mean reward:-500.000\n",
            "mean reward:-500.000\n",
            "mean reward:-500.000\n",
            "mean reward:-500.000\n",
            "mean reward:-500.000\n",
            "mean reward:-500.000\n",
            "mean reward:-500.000\n",
            "mean reward:-500.000\n",
            "mean reward:-500.000\n",
            "mean reward:-500.000\n",
            "mean reward:-500.000\n",
            "mean reward:-500.000\n",
            "mean reward:-500.000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-1627a3373646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     rewards = [train_on_session(*generate_session(), entropy_coef=entropy_coef)\n\u001b[0;32m----> 4\u001b[0;31m                for _ in range(100)]  # generate new sessions\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mentropy_coef\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-1627a3373646>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     rewards = [train_on_session(*generate_session(), entropy_coef=entropy_coef)\n\u001b[0;32m----> 4\u001b[0;31m                for _ in range(100)]  # generate new sessions\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mentropy_coef\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-62a4438b2fc5>\u001b[0m in \u001b[0;36mgenerate_session\u001b[0;34m(t_max)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# action probabilities array aka pi(a|s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Sample action with given probabilities.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-f73507104bb7>\u001b[0m in \u001b[0;36mpredict_probs\u001b[0;34m(states)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# convert states, compute logits, use softmax to get probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTYgRwu_rAwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}